Operating System:
      The operating system is an interface between the user and the machine. 
      It also allows us to communicate with the computer without knowing how to speak the computer’s language.
      It manages computer hardware, software resources, and provides common services for computer programs.
      It manages the computer’s memory, processes, devices, files, and security aspects of the system.i.e. resource manager.
Types of Os:
      Batch OS:
            DE:
            Imagine you are the owner of a photography studio,
            and you have a Batch Operating System in place to process and print photos. 


            This type of operating system does not interact with the computer directly.
            Os --------> cpu.
            There is an operator which takes similar jobs having the same requirement and groups them into batches.
            Os----> batch---> cpu
            It is the responsibility of the operator to sort jobs with similar needs. 
            Jobs---->Os----->batch----->cpu

            Advantages:
                  It is very difficult to guess or know the time required for any job to complete, but processors of the batch systems know how long the job would be when it is in the queue.  
                  It is easy to manage large work repeatedly in batch systems.
            Dis:
                  Batch systems are hard to debug.
                  It is sometimes costly.

    Distributed OS:
            DE:
            Imagine you are part of a team working on a project, 
            and your team members are spread across different locations.To enhance collaboration,
            you decide to use a distributed operating system that allows 
            seamless sharing of computing resources and I/O files.

            It connects multiple computers via a single communication channel.
            (daily life example: people in villagers meet at a large tree).
            Individual systems that communicate via a single channel are regarded as a single entity and They're also known as loosely coupled systems.
      
      Multitasking OS:
            Multitasking Operating System is simply a multiprogramming Operating System with having facility of a Round-Robin Scheduling Algorithm.
            It can run multiple programs simultaneously.
            It comes with proper memory management.

            Dis adv:
            The system gets heated in case of heavy programs multiple times.

            Example:
            Search engine.
      Network OS:
            These systems run on a server and provide the capability to manage data,
                  users, groups, security, applications, and other networking functions. 
            
            A Network Operating System (NOS) is designed to manage and coordinate network resources, 
                  allowing multiple computers to communicate and share resources within a network.
            
            Example:
            File and Print Services:
            
            Users on different computers can share files seamlessly through NetOS.
            Print services allow users to send print jobs to network printers, eliminating the need for direct connections.

            Adv:
            Highly stable centralized servers.
            Security concerns are handled through servers.

            Disadv:
            Servers are costly.
            User has to depend on a central location for most operations

      Real-Time OS:
            The time interval required to process and respond to inputs is very small. 
            This time interval is called response time. 
            Real-time systems are used when there are time requirements that are very strict like missile systems,
            air traffic control systems, robots, etc. 

            Types:
            Hard -> time constraints very strict -> automatic parachute air bag.
            Soft -> streaming services for netflix and prime interms of buffering.

            Adv:
            More output from all the resources.

            Disadv:
            The algorithms are very complex and difficult for the designer to write on.
            Example:
            weapon systems, robots, air traffic control systems.

      Mobile OS:
            Mobile operating systems are specialized operating systems designed
            to run on mobile devices such as smartphones and tablets. 
            They provide a platform for running applications, 
            managing hardware resources, and facilitating communication between various components.
==============
Socket:
      A socket is one endpoint of a two-way communication link between two programs running on the network. 
      A socket is bound to a port number so that the TCP layer can identify the application that data is destined to be sent to.
  Detail Note:
        Normally, a server runs on a specific computer and has a socket that is bound to a specific port number. 
        The server just waits, listening to the socket for a client to make a connection request.

      On the client-side:
        The client knows the hostname of the machine on which the server is running and the port number on which the server is listening. 
        To make a connection request, the client tries to rendezvous with the server on the server's machine and port. The client also needs to identify itself to the server so it binds to a local port number that it will use during this connection. This is usually assigned by the system.

      On the server-side:
          If everything goes well, the server accepts the connection.
          Upon acceptance, the server gets a new socket bound to the same local port and also has its remote endpoint set to the address and port of the client.
          It needs a new socket so that it can continue to listen to the original socket for connection requests while tending to the needs of the connected client.

      result:
            On the client side, if the connection is accepted, a socket is successfully 
            created and the client can use the socket to communicate with the server.


Monolithic Kernel (provides good performance but lots of lines of code):
      It is one of the types of kernel where all operating system services operate in kernel space. It has dependencies between system components. It has huge lines of code which is complex.
      Example : Unix, Linux, Open VMS, XTS-400 etc.
===================================================================================
Difference between process and program and thread? Different types of process. 

Program:
      Program is a set of instructions to perform a certain task. Eg: chrome.exe, notepad.exe

Process:
      Process is an instance of an executing program.
      For example, we write our computer programs in a text file and when we execute this program,
      it becomes a process which performs all the tasks mentioned in the program.

Thread:
      Thread is a path of execution within a process.
      A thread is also known as a lightweight process. 
      The idea is to achieve parallelism by dividing a process into multiple threads.
      For example,Word processor uses multiple threads: one thread to format the text,
      another thread to process inputs.
================================================================================
Virtual Memory:

A computer can address more memory than the amount physically installed on the system.
The main visible advantage of this scheme is that programs can be larger than physical memory.
Virtual memory serves two purposes. First, it allows us to extend the use of physical memory
by using a disk. Second, it allows us to have memory protection, 
because each virtual address is translated to a physical address.
=================================================================================
Page fault:
      A page fault occurs when a program tries to access a portion of memory that is not currently in the computer's RAM, 
      prompting the operating system to retrieve the data from disk storage into RAM.
      Example:
       Imagine your computer's RAM is like a desk where you do your work,
      and the hard drive is like a bookshelf with extra storage. When you need a book (data)
      that's not on your desk,you get up and fetch it from the bookshelf.
==============================================================================
Thrashing:

Thrashing is a condition or a situation when the system is spending a major portion of its time
in servicing the page faults, but the actual processing done is very negligible. 
Relatable content:
The primary cause of thrashing is when the system is overwhelmed with too many processes or tasks
that require more memory than is physically available. 
As a result, the operating system constantly swaps data in and out of the main memory to the 
storage device,leading to a significant decrease in system performance.

Example:
Imagine you're working on a computer with multiple applications open simultaneously,
such as a web browser, a video editing software, and a music player. Each of these applications 
requires a certain amount of memory (RAM) to operate smoothly.

However, if the total memory demand from all these applications exceeds the available physical 
RAM, the operating system starts swapping data between RAM and the hard disk's virtual memory
(swap space). Here's how thrashing could manifest in this scenario.
========================================================================
RAID:
Redundant Array of Independent Disks. 
RAID is a technology that combines multiple hard drives into a single logical unit 
to improve data storage performance, reliability, or a combination of both.
The idea behind RAID is to use several disks together in ways that provide benefits such as 
increased data protection, better performance, or a balance between the two.
RAID (Redundant Array of Independent Disks) is like having backup copies of your important 
files stored in different places on several hard drives or solid-state drives (SSDs).
If one drive stops working, your data is still safe because you have other copies stored on the
other drives. It’s like having a safety net to protect your files from being lost if one of your 
drives breaks down.

There are different RAID levels, each offering different advantages. Here are some common RAID levels explained briefly:

RAID 0 (Striping): Data is split across multiple drives, improving performance because 
                   multiple disks can be read or written to simultaneously. However, there is no
                   redundancy, so if one drive fails, all data is lost.

RAID 1 (Mirroring): Data is duplicated on two or more drives. This provides redundancy, 
                    meaning if one drive fails, the data is still available on the mirrored drive(s). 
                    However, it doesn't offer a performance improvement.

RAID 5 (Striping with Parity): Similar to RAID 0 in terms of performance,
                               but it includes parity information, which allows for data recovery in case one drive fails.
                               RAID 5 requires a minimum of three drives.

RAID 10 (Combination of RAID 1 and RAID 0): It combines mirroring (RAID 1) and striping (RAID 0).
                  Data is both mirrored and striped for both performance and redundancy. 
                  RAID 10 requires a minimum of four drives.

RAID 6 (Striping with Dual Parity): Similar to RAID 5, but with two sets of parity information. 
                  This allows for the system to recover from the failure of up to two drives. 
                  RAID 6 requires a minimum of four drives
==========================================
Deadlock:
      A Deadlock is a situation where each of the computer processes waits for a resource 
      which is being assigned to some other process. In this situation, none of the processes gets
      executed since the resource it needs is held by some other process which is also waiting for
      some other resource to be released.

Key characteristics of deadlocks include:
conditions to achieve a deadlock: Mutual Exclusion,Hold and wait , no preemption and circular wait.
Mutual Exclusion: 
      Processes contend for exclusive control over resources, meaning that once a process holds a 
      resource, others are excluded from using it.

Hold and Wait: 
      Processes hold resources while waiting for others, creating a situation where resources
      are not released until new resources are obtained.

No Preemption: 
      Resources cannot be forcibly taken away from a process; 
      they must be released voluntarily.

Circular Wait: 
      There is a circular chain of processes, each waiting for a resource held by the
      next process in the chain.
====================================================
What is fragmentation? Types of fragmentation.
    As the process is loaded and unloaded from memory, these areas are fragmented into small 
    pieces of memory that cannot be allocated to incoming processes. It is called fragmentation.
    Processes can’t be assigned to memory blocks due to their small size, and the memory
    blocks stay unused. It is also necessary to understand that as programs are loaded and deleted
    from memory, they generate free space or a hole in the memory. These small blocks cannot be 
    allotted to new arriving processes, resulting in inefficient memory use.

            Fragmentation is generally considered undesirable in computer systems because it
            leads to inefficient use of storage space and can degrade performance due to increased
            overhead in managing fragmented resources. Therefore, it is not beneficial and 
            efforts are often made to minimize or eliminate fragmentation where possible.
    Internal Fragmentation:
    ======================
    Definition: Internal fragmentation occurs when allocated memory blocks have unused space within them, 
    which is not utilized by the system.

    Example: Imagine you're packing items into boxes for a move. 
             You have standard-sized boxes, but some items are smaller than the box size. 
             If you use a large box for small items without filling it completely, 
             you experience internal fragmentation because space inside the box is wasted.

    Example: In a file system, if a file is allocated a fixed-size block of memory, and it doesn't fully utilize that space,
    the remaining space is wasted and contributes to internal fragmentation.

    External Fragmentation:
    ======================
    Definition: External fragmentation occurs when free memory blocks are scattered throughout the system,
    making it challenging to allocate a contiguous block of memory for a process or file, even if the total free space is sufficient.

    Example: Imagine a memory system where blocks of memory are allocated and deallocated over time. 
    As blocks are freed up, the memory may become fragmented with small pockets of free space scattered around. 
    Even though the total free space might be enough for a new allocation, finding a contiguous block of that size might be difficult due to external fragmentation.

    Example:
      Imagine a library where books of different sizes are placed on shelves in a somewhat random
      order, depending on when they were returned. Each shelf has limited space, and when a book 
      is borrowed, the shelf space it occupied becomes available. However, the next book returned may 
      not fit perfectly into that space because it's either too big or the remaining space is too small.
      So, even though there might be plenty of total space available across all shelves,
      the library staff can't efficiently use it because the available space is fragmented into small,
      non-contiguous sections. This makes it challenging to store new books in an organized manner 
      without constantly rearranging the existing books or leaving gaps between them.
      This situation mirrors external fragmentation in computer memory management, 
      where available memory is divided into non-contiguous blocks, leading to inefficiency in space utilization.
===================================================================================================================
What is spooling ? 
    Spooling stands for "Simultaneous Peripheral Operations On-Line."
    It is a computer term that refers to a process where data is temporarily stored in a queue to
    be processed by a device or program.
    The primary purpose of spooling is to improve the overall efficiency and performance 
    of a computer system by allowing multiple tasks to be performed concurrently.

    Here's how spooling generally works:
    Input/Output (I/O) Operations: 
        When a computer system needs to perform input or output operations, 
        such as printing a document or reading data from a disk, these operations can be time-consuming.
    Spooling Process: 
        Instead of waiting for the I/O operation to complete before moving on to the next task,
        spooling allows the system to store the data in a temporary queue (spool) while the actual device or program 
        performs the required operation.
    Concurrent Processing: 
        Meanwhile, the system can continue with other tasks or processes, making the overall operation more efficient. 
        This is particularly beneficial in scenarios where I/O operations are slower compared to the processing speed of the CPU.

A common example of spooling is print spooling, 
    where print jobs are stored in a queue (spool) instead of sending them directly to the printer. 
    This allows users to continue working on their tasks while the printer processes the print jobs in the background. 
    Similarly, spooling can be used for tasks like disk I/O, where data is queued for reading or writing operations.

In summary, 
    spooling is a technique used to enhance the efficiency of a computer system by allowing it to overlap input/output operations 
    with other processing tasks. It helps in achieving a more streamlined and concurrent workflow.
==================================================================================================================
What is semaphore and mutex (Differences might be asked)? Define Binary semaphore. 
    Mutexes and Semaphores are kernel resources that provide synchronization services (also known as synchronization primitives). 
    Synchronization is required when multiple processes are executing concurrently,to avoid conflicts between processes using shared resources.
    Semaphore:
        A semaphore is a synchronization primitive used in concurrent programming to control access to shared resources.
        It is an integer variable that, apart from initialization,
        is accessed only through two atomic operations: wait (P) and signal (V).
        Semaphore uses two atomic operations: wait and signal to solve critical section problems.
        Example_by_striver:
                A semaphore is simply a count variable and the queue. Initially, this count variable represents the number of resources available. 
                If there are three restrooms and no person is there, the count is 3 and the queue is empty.
                Whenever a person uses a restroom count is decreased and if the count becomes less than 0 after decrementing, 
                it means no restrooms are available and the remaining persons are added to the queue.
                The person waits in the queue before acquiring the restroom.

                Technical Terms:
                      There are two processes with semaphore. Wait and Signal. Before acquiring a resource you call the waiting process
                      and after using the resource or critical section you call the signal process.
                      The wait process simply decreases and the count signal process simply increases the count.
=========================================================================================================================
Binary Semaphore
      A binary semaphore has values only true and false. 
      We can use binary semaphore as mutex also but additional functionalities like wake up and sleep and queue
      so that’s can be used in place of a mutex it provides additional functionalities like it has a queue and 
      it has the sleep call whenever a process is already acquired a critical section if more process come they go to sleep.

      Additional info:
           A binary semaphore is a specific type of semaphore with only two possible values: 0 and 1. 
          It is often used to implement mutex-like behavior where the semaphore value represents the availability of a resource. 
          If the semaphore value is 1, the resource is available; if it's 0, the resource is currently in use. 
          Binary semaphores are frequently used for mutual exclusion purposes, similar to mutexes.
=================================================================================================================
Mutex:
    A mutex is a synchronization mechanism that provides exclusive access to a shared resource.
    It ensures that only one thread or process can access the critical section at a time.
    A mutex has two states: locked or unlocked.
    A thread attempting to access the critical section must first acquire the mutex (lock it). 
    If the mutex is already locked, the thread is typically blocked until the mutex becomes available. 
    After finishing the critical section, the thread releases the mutex (unlocks it), allowing other threads to acquire it.

===========================================================================================================================
Differences:

Functionality:

Semaphore: Semaphores can be used for signaling, synchronization, and counting.
Mutex: Mutex is designed specifically for providing exclusive access to a shared resource.

Complexity:
Semaphore: Semaphores are more versatile but can be complex to use, especially in scenarios involving multiple processes.
Mutex: Mutexes are simpler and easier to use in situations where mutual exclusion is the primary concern.

Counting:
Semaphore: Semaphores can be initialized with a count greater than one, allowing multiple threads to access the critical section simultaneously (useful for scenarios like producer-consumer problems).
Mutex: Mutexes are generally binary (locked or unlocked), allowing only one thread to access the critical section at a time.
      
======================================================================
Page replacement:(Efficient page replacement algorithms can significantly impact system
      performance by minimizing the number of page faults (instances where a requested page is not
      in memory)and maximizing the effective use of physical memory.)

      Page replacement in operating systems is the mechanism of swapping out pages from memory to disk to accommodate new pages or processes
      In operating systems, page replacement refers to the mechanism where the operating system selects a page in memory (RAM) to be evicted or swapped out to secondary storage (such as disk) to make room for a new page that needs to be loaded into memory. This is necessary when the system's physical memory (RAM) is full and a new page needs to be brought in from secondary storage.

Key points:

Purpose: Page replacement ensures that the most relevant or frequently accessed pages are kept in
memory to optimize performance, while less frequently used pages are swapped out to make space.

Algorithm: Various page replacement algorithms (like FIFO, LRU, LFU, etc.) dictate how the 
operating system decides which page to evict when memory is full.

Criteria: The choice of which page to replace is based on factors like how recently a page was
accessed, how frequently it was accessed (frequency), or a combination of both.

Impact: Efficient page replacement algorithms can significantly impact system performance
by minimizing the number of page faults (instances where a requested page is not in memory) and maximizing the effective use of physical memory.

Page replacement is a fundamental aspect of virtual memory management in modern 
operating systems, enabling them to handle large and complex applications efficiently with 
limited physical memory resources.
=================================
Page replacement algorithms:
A page replacement algorithm is a method used by an operating system to decide which 
page (portion of data or code) in memory (RAM) should be evicted (swapped out) when new pages 
need to be loaded and there is insufficient space in memory. These algorithms aim to optimize 
memory usage by selecting pages to replace based on specific criteria, such as recent usage
patterns or frequency of access, to minimize the impact on system performance.

To know more about these just go through the following link:(VVVIMP)
https://www.geeksforgeeks.org/page-replacement-algorithms-in-operating-systems/

1.FIFo:
This is the simplest page replacement algorithm. 
In this algorithm, the operating system keeps track of all pages in the memory in a queue, 
the oldest page is in the front of the queue. 
When a page needs to be replaced page in the front of the queue is selected for removal. 
      Belady’s Anomaly: Bélády’s anomaly is the name given to the phenomenon where increasing 
      the number of page frames results in an increase in the number of page faults for a given memory
      access pattern.
2.Optimal Page replacement: In this algorithm, pages are replaced which would not be used 
      for the longest duration of time in the future.
3.Least recently used:In this algorithm, page will be replaced which is least recently used. 
4.Most recently used: In this algorithm, page will be replaced which is most recently used.

==================================
Starvation and Aging in Os:

Starving in operating systems occurs when a process is unable to make progress because it is 
consistently ignored or deprioritized by the scheduler in favor of other processes.
Example:
In daily life, "starving" can be compared to waiting in line at a popular food truck
where the vendor consistently serves customers who arrived after you, leaving you without 
the opportunity to place your order and get food. Similarly, in operating systems, a process 
may starve if the scheduler continuously prioritizes other processes, preventing it from executing
and completing its tasks.
**** Another name for deadlock is Circular Waiting. Another name for starvation is Lived lock.
Solution to Starvation: Aging

Aging in operating systems is the process of gradually increasing the priority of a waiting
process over time to prevent starvation.
Aging in operating systems is a technique used in process scheduling to prevent starvation 
by gradually increasing the priority of processes that have been waiting in the ready queue
for an extended period, ensuring fair access to system resources over time. 
This approach helps maintain system efficiency and responsiveness 
by dynamically adjusting process priorities based on their waiting times.
===============================================================================
Why thradhing occurs???
Thrashing occurs in operating systems when the system is overwhelmed by excessive paging activity,
leading to a constant state of high paging and low CPU utilization. This typically happens because
the system is spending more time swapping pages in and out of memory than executing actual instruct
ions, usually due to insufficient physical memory (RAM) relative to the demands of running 
processes. When there isn't enough physical memory to hold all the active processes' working sets,
the operating system spends more time swapping pages between disk and memory, resulting in thrashing.
High degree of multiprogramming(if number of processes keeps on increasing in the memory) ,
lack of frames(if a process is allocated too few frames, then there will be too many 
and too frequent page faults.) causes Thrashing.
====================================================
What is paging and why do we need it?
Paging is a way to manage memory by dividing it into small,
fixed-size blocks called pages, which can be placed anywhere in the physical memory, making it easier to use memory efficiently.
It reduces issues like fragmentation.
It divides the process's memory into fixed-sized blockscalled "pages" and the physical memory 
into blocks of the same size called "frames." Pages can be loaded into any available frame in physical memory.

Example:
Imagine a large book divided into chapters (pages) stored in a library.
Instead of requiring the chapters to be stored in consecutive shelves, the library allows 
chapters to be placed on any available shelf (frames). When you need a specific chapter (page),
the librarian (MMU) uses an index (page table) to find which shelf it is on. If the chapter is 
not currently on a shelf but in a storage room (secondary storage), the librarian retrieves it 
and places it on a shelf,possibly replacing another chapter if there are no free shelves.
========================================================











